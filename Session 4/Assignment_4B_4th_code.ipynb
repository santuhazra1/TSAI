{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EIGHTH.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FontP84Nk6is",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 4B: 4th Network\n",
        "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEQfEhpnlEdC",
        "colab_type": "text"
      },
      "source": [
        "### As we have created our 3rd network. In this Assignment we are going to make at most 3 more changes to achieve two goals.\n",
        "*  ### Further 1st to reduce no of parameter\n",
        "* ### 2nd to increase accuracy to 99.4% \n",
        "\n",
        "### So to achieve the goal lets first install keras library with which we are going to build the model and  import all pakages from keras with which we are going to build the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SJyVpgSxHt4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c88f488-1a07-4638-c0b1-357d599269a8"
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8UpFMsPmLqf",
        "colab_type": "text"
      },
      "source": [
        "### Now we are going to load the pre-shuffled MNIST data. Out of total 70k data we have 60k hand written image as train data and 10k hand written image as test data which is autometically predefined in mnist dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlEUplvoxKAT",
        "colab_type": "code",
        "outputId": "afaac394-82fd-4db1-9447-af115104fb4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDQdgutumNcM",
        "colab_type": "text"
      },
      "source": [
        "### Let's see how our MNIST data looks like in below. Here we can see the 5th hand written digit in X_train as 2 in the displayed image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6Y9Va-xxMXG",
        "colab_type": "code",
        "outputId": "2f185e05-062e-455f-c9a3-119623f4210d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[11])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f12b5e9e7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0aYZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzZem2W5mRro",
        "colab_type": "text"
      },
      "source": [
        "### Now we have to shape all the image size in test and train so that we can apply out CNN model based on the input image shape and we will not have any problem while testing with test images with same shape. Let's shape train and test data to (28 , 28 , 1) which is going to be the input dimension of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxDZxPhhxOgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajDwgVEemZeH",
        "colab_type": "text"
      },
      "source": [
        "### Here in the below section we are going to scale our pixel values to 0-1 as grey scale pixel lies between 0-255. So to train a robust we must scale the pixcel values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HzMqbTnxQQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlI0tEG0mf4i",
        "colab_type": "text"
      },
      "source": [
        "### Let's see how Y looks like as Y values are the actual no corrosponding to an hand written image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LdYiW6ixR9e",
        "colab_type": "code",
        "outputId": "966340f5-2312-4184-fd19-4a27974bc0fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYLeJa_hmsOA",
        "colab_type": "text"
      },
      "source": [
        "### Now we have to convert Y values from 1-dimentional class matrix to 10 dimentional class matrix so that we can predict out of 10 class which class it is predicting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5a5e2453-0fed-4f29-ba0d-8d473222da88",
        "id": "8ZtGgZrimqMR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)\n",
        "Y_train[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iENbAm9Cm6eJ",
        "colab_type": "text"
      },
      "source": [
        "### Now let's build our CNN model which we are going to apply on our train data for training the model later. Here we are going to use 2-D convolution, maxpooling, dropout, batch normalization and softmax activation function to get output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDpXf4YQxXRm",
        "colab_type": "code",
        "outputId": "d1f5344b-e70b-44ef-af16-e910cb554be6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Convolution2D(12, 3, 3, activation='relu', input_shape=(28,28,1)))#26\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(15, 3, 3, activation='relu'))#24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(20, 3, 3, activation='relu'))#22\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(12, 1, 1, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))#11\n",
        "\n",
        "model.add(Convolution2D(12, 3, 3, activation='relu'))#9\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(15, 3, 3, activation='relu'))#7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(20, 3, 3, activation='relu'))#5\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu'))#5\n",
        "model.add(Convolution2D(10, 5, 5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(15, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (1, 1), activation=\"relu\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(15, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (5, 5))`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLaL_dStCqVG",
        "colab_type": "text"
      },
      "source": [
        "### Here in the model summary we can see that out total no of model parameter is 18.6k which is way less than our 1st vanilla model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ytv2s75P6iG",
        "colab_type": "code",
        "outputId": "be141aff-5018-4227-edb8-e78958fe546b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_64 (Conv2D)           (None, 26, 26, 12)        120       \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 26, 26, 12)        48        \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 26, 26, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_65 (Conv2D)           (None, 24, 24, 15)        1635      \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 24, 24, 15)        60        \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 24, 24, 15)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_66 (Conv2D)           (None, 22, 22, 20)        2720      \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 22, 22, 20)        80        \n",
            "_________________________________________________________________\n",
            "dropout_45 (Dropout)         (None, 22, 22, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_67 (Conv2D)           (None, 22, 22, 12)        252       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 11, 11, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_68 (Conv2D)           (None, 9, 9, 12)          1308      \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 9, 9, 12)          48        \n",
            "_________________________________________________________________\n",
            "dropout_46 (Dropout)         (None, 9, 9, 12)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_69 (Conv2D)           (None, 7, 7, 15)          1635      \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 7, 7, 15)          60        \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 7, 7, 15)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_70 (Conv2D)           (None, 5, 5, 20)          2720      \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 5, 5, 20)          80        \n",
            "_________________________________________________________________\n",
            "dropout_48 (Dropout)         (None, 5, 5, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_71 (Conv2D)           (None, 5, 5, 10)          210       \n",
            "_________________________________________________________________\n",
            "conv2d_72 (Conv2D)           (None, 1, 1, 10)          2510      \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 13,486\n",
            "Trainable params: 13,298\n",
            "Non-trainable params: 188\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMBJDl_DC2tI",
        "colab_type": "text"
      },
      "source": [
        "### Lets's compile the model with adam optimizer ,loss as cross entropy and validation matrix as accuracy. After compiling the model we are going to train it with training data and let's see how much training accuracy we get after training completion. Here we have used batch size as 512 and total no of epoch 40 to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow_e7bRoSw-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.32 * epoch), 10)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMlJztxcSW1g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2771
        },
        "outputId": "76e91457-4699-4a92-ecda-b71f04eb9245"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=512, epochs=40, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 10s 160us/step - loss: 0.4608 - acc: 0.8560 - val_loss: 0.0858 - val_acc: 0.9703\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022727273.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0874 - acc: 0.9727 - val_loss: 0.0522 - val_acc: 0.9833\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018292683.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0634 - acc: 0.9800 - val_loss: 0.0446 - val_acc: 0.9861\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015306122.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0544 - acc: 0.9829 - val_loss: 0.0375 - val_acc: 0.9883\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013157895.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0456 - acc: 0.9862 - val_loss: 0.0378 - val_acc: 0.9883\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011538462.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0438 - acc: 0.9861 - val_loss: 0.0318 - val_acc: 0.9904\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010273973.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0397 - acc: 0.9878 - val_loss: 0.0323 - val_acc: 0.9893\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009259259.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0367 - acc: 0.9882 - val_loss: 0.0277 - val_acc: 0.9915\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008426966.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0360 - acc: 0.9886 - val_loss: 0.0353 - val_acc: 0.9886\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007731959.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0326 - acc: 0.9897 - val_loss: 0.0264 - val_acc: 0.9914\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007142857.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0316 - acc: 0.9901 - val_loss: 0.0274 - val_acc: 0.9919\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0006637168.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0306 - acc: 0.9906 - val_loss: 0.0266 - val_acc: 0.9922\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006198347.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0298 - acc: 0.9905 - val_loss: 0.0284 - val_acc: 0.9916\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005813953.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0296 - acc: 0.9905 - val_loss: 0.0244 - val_acc: 0.9933\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005474453.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0282 - acc: 0.9909 - val_loss: 0.0274 - val_acc: 0.9923\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005172414.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0276 - acc: 0.9913 - val_loss: 0.0244 - val_acc: 0.9926\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0004901961.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0260 - acc: 0.9917 - val_loss: 0.0247 - val_acc: 0.9933\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004658385.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0257 - acc: 0.9917 - val_loss: 0.0258 - val_acc: 0.9926\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.000443787.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0260 - acc: 0.9917 - val_loss: 0.0253 - val_acc: 0.9934\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0004237288.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0255 - acc: 0.9916 - val_loss: 0.0246 - val_acc: 0.9937\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004054054.\n",
            "60000/60000 [==============================] - 4s 64us/step - loss: 0.0254 - acc: 0.9915 - val_loss: 0.0249 - val_acc: 0.9936\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000388601.\n",
            "60000/60000 [==============================] - 4s 64us/step - loss: 0.0232 - acc: 0.9930 - val_loss: 0.0247 - val_acc: 0.9935\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003731343.\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.0242 - acc: 0.9919 - val_loss: 0.0244 - val_acc: 0.9929\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003588517.\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.0236 - acc: 0.9924 - val_loss: 0.0234 - val_acc: 0.9934\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003456221.\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.0223 - acc: 0.9930 - val_loss: 0.0236 - val_acc: 0.9931\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003333333.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0222 - acc: 0.9929 - val_loss: 0.0250 - val_acc: 0.9930\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003218884.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0216 - acc: 0.9932 - val_loss: 0.0261 - val_acc: 0.9929\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003112033.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0223 - acc: 0.9929 - val_loss: 0.0233 - val_acc: 0.9937\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0003012048.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0228 - acc: 0.9920 - val_loss: 0.0245 - val_acc: 0.9932\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002918288.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0213 - acc: 0.9927 - val_loss: 0.0217 - val_acc: 0.9937\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002830189.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0210 - acc: 0.9929 - val_loss: 0.0235 - val_acc: 0.9936\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002747253.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0220 - acc: 0.9930 - val_loss: 0.0245 - val_acc: 0.9927\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0002669039.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0210 - acc: 0.9935 - val_loss: 0.0235 - val_acc: 0.9932\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002595156.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0194 - acc: 0.9937 - val_loss: 0.0229 - val_acc: 0.9933\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0002525253.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0210 - acc: 0.9933 - val_loss: 0.0235 - val_acc: 0.9933\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002459016.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0195 - acc: 0.9939 - val_loss: 0.0236 - val_acc: 0.9937\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002396166.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0195 - acc: 0.9936 - val_loss: 0.0238 - val_acc: 0.9937\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002336449.\n",
            "60000/60000 [==============================] - 4s 62us/step - loss: 0.0192 - acc: 0.9937 - val_loss: 0.0235 - val_acc: 0.9941\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002279635.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0196 - acc: 0.9933 - val_loss: 0.0254 - val_acc: 0.9932\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002225519.\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.0192 - acc: 0.9938 - val_loss: 0.0258 - val_acc: 0.9930\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f124a0a0b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLxlW9ufyQiO",
        "colab_type": "code",
        "outputId": "286808ca-ff45-41da-cad6-87befc6b39bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.025777660852935515, 0.993]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufi2m-7knm_7",
        "colab_type": "text"
      },
      "source": [
        "#### In this model first we can see we have achieved maximum of 99.41% validation accuracy in 38th epoch out of 40. As we can see we have introduced learning rate optimization, increased batch size to 512. Total global receptive field for this network is 25. Here are few observations:\n",
        "* #### Here we can see we have increased no of parameter from 12.5k to 13.5k from our previous model to achieve 99.4% accuracy.\n",
        "* #### Also, we can see after introducing learning rate optimization accuracy increased from 99.29 to 99.41. It is going to reduce learning rate with each epoch.\n",
        "* #### We have also increased batch size from 128 to 512 which has introduced a lot of variation in each batch because of which model has performed better.\n",
        "\n",
        "#### Finally we have reached 99.4% accuracy with 13.5k parameter."
      ]
    }
  ]
}