{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sixth DNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FontP84Nk6is",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 4B: 2nd Network\n",
        "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEQfEhpnlEdC",
        "colab_type": "text"
      },
      "source": [
        "### As we have created our first vanilla network. In this Assignment we are going to make at most 3 changes to achieve two goals.\n",
        "*  ### 1st to reduce no of parameter\n",
        "* ### 2nd to increase accuracy \n",
        "\n",
        "### So to achieve the goal lets fist install keras library with which we are going to build the model and  import all pakages from keras with which we are going to build the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SJyVpgSxHt4",
        "colab_type": "code",
        "outputId": "6e01a9a6-124c-4015-b1a8-e123672ad42c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8UpFMsPmLqf",
        "colab_type": "text"
      },
      "source": [
        "### Now we are going to load the pre-shuffled MNIST data. Out of total 70k data we have 60k hand written image as train data and 10k hand written image as test data which is autometically predefined in mnist dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlEUplvoxKAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDQdgutumNcM",
        "colab_type": "text"
      },
      "source": [
        "### Let's see how our MNIST data looks like in below. Here we can see the 5th hand written digit in X_train as 2 in the displayed image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6Y9Va-xxMXG",
        "colab_type": "code",
        "outputId": "f829e250-9b78-4056-f303-94bc0cab435d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[10])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7eff023aa240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQRF+ICjCDwRF+IGg/gKDjjqqTRCtawAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzZem2W5mRro",
        "colab_type": "text"
      },
      "source": [
        "### Now we have to shape all the image size in test and train so that we can apply out CNN model based on the input image shape and we will not have any problem while testing with test images with same shape. Let's shape train and test data to (28 , 28 , 1) which is going to be the input dimension of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxDZxPhhxOgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajDwgVEemZeH",
        "colab_type": "text"
      },
      "source": [
        "### Here in the below section we are going to scale our pixel values to 0-1 as grey scale pixel lies between 0-255. So to train a robust we must scale the pixcel values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HzMqbTnxQQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlI0tEG0mf4i",
        "colab_type": "text"
      },
      "source": [
        "### Let's see how Y looks like as Y values are the actual no corrosponding to an hand written image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LdYiW6ixR9e",
        "colab_type": "code",
        "outputId": "494b8979-fbe4-4136-a2f5-147ec1d6c598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYLeJa_hmsOA",
        "colab_type": "text"
      },
      "source": [
        "### Now we have to convert Y values from 1-dimentional class matrix to 10 dimentional class matrix so that we can predict out of 10 class which class it is predicting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1304ec8b-51de-4452-fab4-0d1e72b24cba",
        "id": "8ZtGgZrimqMR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)\n",
        "Y_train[:10]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iENbAm9Cm6eJ",
        "colab_type": "text"
      },
      "source": [
        "### Now let's build our CNN model which we are going to apply on our train data for training the model later. Here we are going to use 2-D convolution, maxpooling, dropout and softmax activation function to get output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDpXf4YQxXRm",
        "colab_type": "code",
        "outputId": "a7f3c9c1-2e24-4bb4-bcf6-252c0ac45a37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Convolution2D(10, 3, 3, activation='relu', input_shape=(28,28,1)))#26\n",
        "\n",
        "model.add(Convolution2D(20, 3, 3, activation='relu'))#24\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(30, 3, 3, activation='relu'))#22\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))#11\n",
        "\n",
        "model.add(Convolution2D(10, 3, 3, activation='relu'))#9\n",
        "\n",
        "model.add(Convolution2D(20, 3, 3, activation='relu'))#7\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(30, 3, 3, activation='relu'))#5\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu'))#5\n",
        "model.add(Convolution2D(10, 5, 5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(30, (3, 3), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(30, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (5, 5))`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLaL_dStCqVG",
        "colab_type": "text"
      },
      "source": [
        "### Here in the model summary we can see that out total no of model parameter is 18.6k which is way less than our 1st vanilla model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ytv2s75P6iG",
        "colab_type": "code",
        "outputId": "0e0fbeed-8590-44e8-fa2a-d998542d81e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_37 (Conv2D)           (None, 26, 26, 10)        100       \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 24, 24, 20)        1820      \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 24, 24, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 22, 22, 30)        5430      \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 22, 22, 30)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 22, 22, 10)        310       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 11, 11, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 9, 9, 10)          910       \n",
            "_________________________________________________________________\n",
            "conv2d_42 (Conv2D)           (None, 7, 7, 20)          1820      \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 7, 7, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 5, 5, 30)          5430      \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 5, 5, 30)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 5, 5, 10)          310       \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 1, 1, 10)          2510      \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 18,640\n",
            "Trainable params: 18,640\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMBJDl_DC2tI",
        "colab_type": "text"
      },
      "source": [
        "### Lets's compile the model with sgd optimizer ,loss as cross entropy and validation matrix as accuracy. After compiling the model we are going to train it with training data and let's see how much training accuracy we get after training completion. Here we have used batch size as 64 and total no of epoch 20 to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2IicGJ4x3Be",
        "colab_type": "code",
        "outputId": "3b9b7e08-7677-4fc4-8621-f7b35df6f50f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='sgd',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=64, nb_epoch=20, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 1.5293 - acc: 0.4677 - val_loss: 0.4019 - val_acc: 0.8816\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.3623 - acc: 0.8882 - val_loss: 0.2003 - val_acc: 0.9392\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.2321 - acc: 0.9290 - val_loss: 0.1398 - val_acc: 0.9569\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.1766 - acc: 0.9462 - val_loss: 0.1145 - val_acc: 0.9632\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.1505 - acc: 0.9540 - val_loss: 0.0975 - val_acc: 0.9687\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.1335 - acc: 0.9592 - val_loss: 0.0899 - val_acc: 0.9723\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.1181 - acc: 0.9635 - val_loss: 0.0729 - val_acc: 0.9771\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.1095 - acc: 0.9671 - val_loss: 0.0732 - val_acc: 0.9759\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.1012 - acc: 0.9689 - val_loss: 0.0666 - val_acc: 0.9794\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0945 - acc: 0.9714 - val_loss: 0.0587 - val_acc: 0.9804\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0899 - acc: 0.9728 - val_loss: 0.0568 - val_acc: 0.9833\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0835 - acc: 0.9740 - val_loss: 0.0614 - val_acc: 0.9813\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.0793 - acc: 0.9759 - val_loss: 0.0567 - val_acc: 0.9824\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.0756 - acc: 0.9765 - val_loss: 0.0532 - val_acc: 0.9810\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0734 - acc: 0.9769 - val_loss: 0.0507 - val_acc: 0.9838\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0717 - acc: 0.9782 - val_loss: 0.0445 - val_acc: 0.9865\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0672 - acc: 0.9789 - val_loss: 0.0457 - val_acc: 0.9853\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.0669 - acc: 0.9791 - val_loss: 0.0450 - val_acc: 0.9845\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.0632 - acc: 0.9804 - val_loss: 0.0423 - val_acc: 0.9865\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.0627 - acc: 0.9803 - val_loss: 0.0405 - val_acc: 0.9869\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efef02fc358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLxlW9ufyQiO",
        "colab_type": "code",
        "outputId": "0eb2f38c-7ea0-4e8f-bd3a-7d406f98a612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.040513473138096744, 0.9869]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufi2m-7knm_7",
        "colab_type": "text"
      },
      "source": [
        "#### In this first we can see we have achieved maximum of 98.69% validation accuracy in 20th epoch out of 20. As we can see we have introduced dropout, increased batch size to 64 and changed adam optimizer to sgd. Total global receptive field for this network is 25. Here are few observations:\n",
        "* #### Here we can see we have reduced no of parameter from 125k to 18.6 k from our previous model with a cost of 0.55% reduction of validation accuracy. Since no filter in each layer is less which is going to capture less texture and gradient information from last model, though computational cost is very less. \n",
        "* #### Also, we can see after introducing dropout overfitting reduced between training and validation accuracy at a great extent. \n",
        "* #### We have also increased batch size from 32 to 64 which decreased training time from 15s per epoch to 6s per epoch. Also, increasing batch size is going to introduce lot of variation in each batch because of which model is going to perform better.\n",
        "\n",
        "#### As we have achieved this result in this network, in next few network we will try to reduce parameters and achieve more accuracy by introducing more advance techniques."
      ]
    }
  ]
}