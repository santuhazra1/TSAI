{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_4B_1st_code.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FontP84Nk6is",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 4B: Vanilla Network\n",
        "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEQfEhpnlEdC",
        "colab_type": "text"
      },
      "source": [
        "### In this Assignment we are going to run a CNN model on top of MNIST data by using only 3 × 3 conv, 1 × 1 conv and maxpooling as this is going to be our first vanilla network, on top of which we will make some change to improve it. \n",
        "### So to achieve the goal lets fist install keras library with which we are going to build the model and  import all pakages from keras with which we are going to build the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SJyVpgSxHt4",
        "colab_type": "code",
        "outputId": "644fc795-f57e-417a-ac69-9b5a77a85d61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8UpFMsPmLqf",
        "colab_type": "text"
      },
      "source": [
        "### Now we are going to load the pre-shuffled MNIST data. Out of total 70k data we have 60k hand written image as train data and 10k hand written image as test data which is autometically predefined in mnist dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlEUplvoxKAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDQdgutumNcM",
        "colab_type": "text"
      },
      "source": [
        "### Let's see how our MNIST data looks like in below. Here we can see the 5th hand written digit in X_train as 2 in the displayed image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6Y9Va-xxMXG",
        "colab_type": "code",
        "outputId": "fb7aa04c-bc7e-4951-9103-5fc1f8d71bd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[8])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fdb2b0fc048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npaIMv1SU4ZeKMvxSUf8Pppy63sRz9OcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzZem2W5mRro",
        "colab_type": "text"
      },
      "source": [
        "### Now we have to shape all the image size in test and train so that we can apply out CNN model based on the input image shape and we will not have any problem while testing with test images with same shape. Let's shape train and test data to (28 , 28 , 1) which is going to be the input dimension of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxDZxPhhxOgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajDwgVEemZeH",
        "colab_type": "text"
      },
      "source": [
        "### Here in the below section we are going to scale our pixel values to 0-1 as grey scale pixel lies between 0-255. So to train a robust we must scale the pixcel values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HzMqbTnxQQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlI0tEG0mf4i",
        "colab_type": "text"
      },
      "source": [
        "### Let's see how Y looks like as Y values are the actual no corrosponding to an hand written image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LdYiW6ixR9e",
        "colab_type": "code",
        "outputId": "4e99f0ef-d2ea-435d-afae-f481572cfc9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYLeJa_hmsOA",
        "colab_type": "text"
      },
      "source": [
        "### Now we have to convert Y values from 1-dimentional class matrix to 10 dimentional class matrix so that we can predict out of 10 class which class it is predicting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8556115b-9a5a-46ce-cbf9-349ab050f4a0",
        "id": "8ZtGgZrimqMR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)\n",
        "Y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iENbAm9Cm6eJ",
        "colab_type": "text"
      },
      "source": [
        "### Now let's build our CNN model which we are going to apply on our train data for training the model later. Here we are using 2-D convolution, maxpooling and softmax activation function to get output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDpXf4YQxXRm",
        "colab_type": "code",
        "outputId": "c0b52abc-a4ca-4263-b00d-420918800d4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1)))#26\n",
        "\n",
        "model.add(Convolution2D(64, 3, 3, activation='relu'))#24\n",
        "\n",
        "model.add(Convolution2D(64, 3, 3, activation='relu'))#22\n",
        "\n",
        "model.add(Convolution2D(32, 1, 1, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))#11\n",
        "\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu'))#9\n",
        "\n",
        "model.add(Convolution2D(64, 3, 3, activation='relu'))#7\n",
        "\n",
        "model.add(Convolution2D(64, 3, 3, activation='relu'))#5\n",
        "\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu'))#5\n",
        "model.add(Convolution2D(10, 5, 5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), activation=\"relu\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (5, 5))`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-mzjHf6BvrD",
        "colab_type": "text"
      },
      "source": [
        "### Here in the model summary we can see that out total no of model parameter is 125k."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ytv2s75P6iG",
        "colab_type": "code",
        "outputId": "65661084-d682-4ed9-ee13-fc2bec5d39e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_52 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_54 (Conv2D)           (None, 22, 22, 64)        36928     \n",
            "_________________________________________________________________\n",
            "conv2d_55 (Conv2D)           (None, 22, 22, 32)        2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_56 (Conv2D)           (None, 9, 9, 32)          9248      \n",
            "_________________________________________________________________\n",
            "conv2d_57 (Conv2D)           (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "conv2d_58 (Conv2D)           (None, 5, 5, 64)          36928     \n",
            "_________________________________________________________________\n",
            "conv2d_59 (Conv2D)           (None, 5, 5, 10)          650       \n",
            "_________________________________________________________________\n",
            "conv2d_60 (Conv2D)           (None, 1, 1, 10)          2510      \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 125,656\n",
            "Trainable params: 125,656\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnLZiFI7B9Yp",
        "colab_type": "text"
      },
      "source": [
        "### Lets's compile the model with adam optimizer ,loss as cross entropy and validation matrix as accuracy. After compiling the model we are going to train it with training data and let's see how much training accuracy we get after training completion. Here we have used batch size as 32 and total no of epoch 20 to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2IicGJ4x3Be",
        "colab_type": "code",
        "outputId": "17001ef7-314f-473a-82f9-63866ecc99bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=20, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0563 - acc: 0.9830 - val_loss: 0.0385 - val_acc: 0.9872\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 15s 252us/step - loss: 0.0389 - acc: 0.9881 - val_loss: 0.0332 - val_acc: 0.9884\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 15s 246us/step - loss: 0.0301 - acc: 0.9912 - val_loss: 0.0274 - val_acc: 0.9914\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 15s 246us/step - loss: 0.0272 - acc: 0.9913 - val_loss: 0.0412 - val_acc: 0.9884\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 15s 246us/step - loss: 0.0232 - acc: 0.9921 - val_loss: 0.0325 - val_acc: 0.9904\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0187 - acc: 0.9939 - val_loss: 0.0337 - val_acc: 0.9914\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0176 - acc: 0.9942 - val_loss: 0.0285 - val_acc: 0.9919\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 15s 245us/step - loss: 0.0145 - acc: 0.9954 - val_loss: 0.0335 - val_acc: 0.9898\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 15s 245us/step - loss: 0.0133 - acc: 0.9956 - val_loss: 0.0276 - val_acc: 0.9920\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 15s 247us/step - loss: 0.0111 - acc: 0.9962 - val_loss: 0.0322 - val_acc: 0.9917\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 15s 246us/step - loss: 0.0115 - acc: 0.9963 - val_loss: 0.0325 - val_acc: 0.9906\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 16s 263us/step - loss: 0.0106 - acc: 0.9967 - val_loss: 0.0344 - val_acc: 0.9912\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 15s 246us/step - loss: 0.0104 - acc: 0.9967 - val_loss: 0.0352 - val_acc: 0.9921\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 15s 245us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 0.0395 - val_acc: 0.9915\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 15s 247us/step - loss: 0.0101 - acc: 0.9967 - val_loss: 0.0450 - val_acc: 0.9907\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 15s 247us/step - loss: 0.0075 - acc: 0.9977 - val_loss: 0.0464 - val_acc: 0.9913\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 15s 250us/step - loss: 0.0085 - acc: 0.9974 - val_loss: 0.0383 - val_acc: 0.9920\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 15s 254us/step - loss: 0.0089 - acc: 0.9972 - val_loss: 0.0426 - val_acc: 0.9909\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 15s 250us/step - loss: 0.0068 - acc: 0.9979 - val_loss: 0.0427 - val_acc: 0.9924\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 15s 250us/step - loss: 0.0090 - acc: 0.9972 - val_loss: 0.0487 - val_acc: 0.9904\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdb2bab52b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLxlW9ufyQiO",
        "colab_type": "code",
        "outputId": "14e528f1-43d3-47df-8ebf-ab3a62b90586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.048743565161659896, 0.9904]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufi2m-7knm_7",
        "colab_type": "text"
      },
      "source": [
        "#### In this first we can see we have achieved maximum of 99.24% validation accuracy in 19th epoch out of 20. Here we have only used 3 × 3 conv, 1 × 1 conv and maxpooling layer. Total global receptive field for this network is 25. Here we have used atleast 3 layers of convolution before using maxpooling as MNIST is a very small dataset and there is not much texture and gradient to learn from this data.. So, 3 layers of conv layers are enough. Aslo, for the same reason we have only used 32 filters in the first layers. \n",
        "#### As we have achieved this result in this network, in next few network we will try to reduce parameters and achieve more accuracy."
      ]
    }
  ]
}